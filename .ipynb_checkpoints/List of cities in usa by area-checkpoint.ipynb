{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll be scrapping the data from \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_area\" which has top US cities  by the area. For this I have used BeautifulSoup  a python package used to parse the HTML and XML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definig the url and calling BeautifulSoup\n",
    "url=\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_area\"\n",
    "response= requests.get(url)\n",
    "soup= BeautifulSoup(response.text,'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we'll define the table and rows and columns of that table.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rank', 'City', 'State', 'Land area (sqmi)', 'Land area (km2)', 'Water area (sqmi)', 'Water area (km2)', 'Total area (sqmi)', 'Total area (km2)', 'Population (2010)']\n"
     ]
    }
   ],
   "source": [
    "table = soup.find('table',{'class':'wikitable sortable'})\n",
    "rows= table.find_all('tr')\n",
    "columns=[v.text.replace('\\n','').replace('\\xa0','') for v in rows[0].find_all('th')]\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(columns=columns)\n",
    "for i in range(1, len(rows)):\n",
    "    tds= rows[i].find_all('td')\n",
    "    if len(tds)== 9:\n",
    "        content=[tds[0].text, tds[1].text, tds[2].text, \n",
    "                 tds[3].text.replace('\\n',''), tds[4].text, tds[5].text.replace('\\n',''),\n",
    "                 tds[6].text, tds[7].text.replace('\\n',''), tds[8].text, tds[9].text.replace('\\n','')]\n",
    "    else:\n",
    "        content=[td.text.replace('\\n','') for td in tds]\n",
    "    df = df.append(pd.Series(content, index=columns), ignore_index=True)  \n",
    "    df.to_csv('Cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
